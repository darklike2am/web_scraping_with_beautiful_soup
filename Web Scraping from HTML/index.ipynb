{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7d132b",
   "metadata": {},
   "source": [
    "    ## Welcome to WebScraping with Beautiful Soup\n",
    "    I learnt this today 29/8/2022, and as part of my accountability process, i will be sharing them with you guys.\n",
    "    The first thing I learnt to do was to install beautifulsoup using \"pip install\" command. Along side, I installed request. \n",
    "    below is a link to a YT video on that. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f890ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After succesfully installing the packages, you'd import the to your code as stated below:\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# There are two ways of web scraping I learn't today\n",
    "# The first is from a local webpage\n",
    "# The second is from a URL.\n",
    "# Let's do the first one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7f04f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"bold-text\">\n",
      "            I have bad news <br/>for you\n",
      "        </p>, <p class=\"regular-text\">\n",
      "            The page you are looking for might be removed or is temporarily unavailable\n",
      "        </p>]\n"
     ]
    }
   ],
   "source": [
    "# The first thing you want to do is locate your local webpage,\n",
    "# for the sake of ease, put it in the same folder as your notebook\n",
    "# so you can call it easily into your code.\n",
    "\n",
    "with open('index.html', 'r') as html_file:\n",
    "    content= html_file.read()\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    error_tags= soup.find_all('p')\n",
    "    print(error_tags)\n",
    "#the page here is an old html file i created when i was practicing front-end dev    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9dcef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The second one I learnt, is webscraping from a URL and I will be demonstrating it below\n",
    "\n",
    "def applePriceTraker():\n",
    "    apple_text=requests.get('https://finance.yahoo.com/quote/AAPL/').text\n",
    "    apple_soup=BeautifulSoup(apple_text, 'lxml')\n",
    "    price=apple_soup.find_all('div', {'class':'D(ib) Va(m) Maw(65%) Ov(h)'})[0].find('fin-streamer', class_='Fw(b) Fz(36px) Mb(-4px) D(ib)').text\n",
    "    print(price)\n",
    "    \n",
    "#while True:\n",
    "    applePriceTraker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f173d28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5071bcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
